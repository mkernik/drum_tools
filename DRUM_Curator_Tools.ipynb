{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkernik/drum_tools/blob/colab_contact_author/DRUM_Curator_Tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqv_-9LHz7dt"
      },
      "source": [
        "\n",
        "# Automation Tools for DRUM Curators\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Notebook created by:**\n",
        "\n",
        "*   Melinda Kernik, University of Minnesota Libraries\n",
        "*   Valerie Collins, University of Minnesota Libraries\n",
        "\n",
        "**Contact**: datarepo@umn.edu\n",
        "\n",
        "The code in this notebook is intended for data curators working with records associated with the [Data Repository for the University of Minnesota](https://conservancy.umn.edu/drum). More information about this code can be found in the main [GitHub repository](https://github.com/mkernik/drum_tools)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents\n",
        "\n",
        "<small>*Only the \"Start here\" section is a mandatory step in using this notebook. After this step is completed, any of the \"Create\" sections can be run in any order.*</small>\n",
        "\n",
        "1.   Start here\n",
        "  -   Create Curator Log\n",
        "  -   Create Readme File\n",
        "  -   Create XML File\n",
        "\n",
        "<small>*External resources related to these tools are linked from the following sections:*</small>\n",
        "2.   Known Issues and Limitations\n",
        "3.   Download All Files from Record"
      ],
      "metadata": {
        "id": "XD9jwmYZlIkK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwsv1j4011_m"
      },
      "source": [
        "## Start Here\n",
        "\n",
        "\n",
        "---\n",
        "Activate this notebook by running the cell below. You must have this notebok open in Colab to do this.\n",
        "\n",
        "> An input box for text will appear once the notebook has activated. Copy in the **handle** for a DRUM record into this input box, and then hit the enter key. The notebook will now remember this handle, and will use it when you run any of the code blocks below.\n",
        "\n",
        "> If you enter an incorrect value for a handle, the code below will not run, but you can enter a new value by running this starting code block again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmOF4rz_1KUB"
      },
      "outputs": [],
      "source": [
        "handle_url = input()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYvpjDCV5j6t"
      },
      "source": [
        "## CREATE CURATOR LOG\n",
        "\n",
        "\n",
        "---\n",
        "Run this block of code to create a curator log that will populate with the existing information on the record. By default, this file will be saved to your Downloads folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clGwanfA7btF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "script name: metadata_log.py\n",
        "\n",
        "inputs: DRUM handle URL (i.e. https://hdl.handle.net/11299/226188 or https://conservancy.umn.edu/handle/11299/226188)\n",
        "output: Text file with curator log template filled out with metadata from the submission\n",
        "\n",
        "description: This script generates a metadata log based on the information \n",
        "entered by researchers during the submission process to the Data Repository \n",
        "for the University of Minnesota (DRUM) and the original files uploaded.\n",
        "\n",
        "last modified: July 2022\n",
        "author: Melinda Kernik\n",
        "\"\"\"\n",
        "\n",
        "import urllib.request\n",
        "import math\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        " \n",
        "def convert_size(size_bytes):\n",
        "    \"\"\"Convert file size in bytes to a more human readable format\"\"\"\n",
        "\n",
        "    if size_bytes == 0:\n",
        "        return \"0B\"\n",
        "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
        "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
        "    p = math.pow(1024, i)\n",
        "    s = round(size_bytes / p, 2)\n",
        "    return \"%s %s\" % (s, size_name[i])\n",
        "\n",
        "    \n",
        "\n",
        "#Use the handle URL to construct a URL to get to the Dspace endpoint for the item  \n",
        "handle_split = handle_url.split (\"/\") [-2:]\n",
        "handle = str(handle_split[0]) + \"/\" + str(handle_split[1])\n",
        "url = \"https://conservancy.umn.edu/rest/handle/\" + handle\n",
        "\n",
        "def validate_input(url):\n",
        "  \"\"\"Test whether the API for the DRUM item can be opened\"\"\"\n",
        "  valid_url = False\n",
        "  try:\n",
        "    response = urllib.request.urlopen(url)\n",
        "    valid_url = True\n",
        "    return valid_url\n",
        "  except Exception as e:\n",
        "    print (url + \" could not be opened. (\" + str(e) + \")\")\n",
        "\n",
        "#If the API can be reached, continue generating a log file\n",
        "valid = validate_input(url)\n",
        "if valid:\n",
        "  response = urllib.request.urlopen(url)\n",
        "  #Read in the content at the endpoint and get the internal id for the item\n",
        "  presoup = BeautifulSoup(response, 'lxml')\n",
        "  item_info = presoup.p.text\n",
        "  item_dict = eval(item_info.replace('null', '\"null\"'))\n",
        "\n",
        "  internal_id = item_dict[\"id\"]\n",
        "\n",
        "\n",
        "  ###Get item bitstream information from the submission\n",
        "\n",
        "  #Read in the content at the bitstream endpoint. Default limit is 20 items per page.  \n",
        "  #Extended to 250 to account for larger data submissions.\n",
        "  url = \"https://conservancy.umn.edu/rest/items/\" + str(internal_id) + \"/bitstreams?limit=250\"\n",
        "  response = urllib.request.urlopen(url)\n",
        "  item_soup = BeautifulSoup(response, 'lxml')\n",
        "  bitstream = item_soup.p.text\n",
        "  list_bitstream = eval(bitstream.replace('null', '\"null\"'))\n",
        "\n",
        "  #Create the item bitstream section of the log. \n",
        "  #Note: Embargoed files are not visible through the API\n",
        "  bitstream_string = \"\"\n",
        "  for x in list_bitstream:\n",
        "      if x['bundleName'] == \"ORIGINAL\":\n",
        "          bitstream_string += (x['name'] + \" (\" + convert_size(x['sizeBytes']) + \")\\n\")\n",
        "\n",
        "\n",
        "  ###Get metadata from the submission\n",
        "  #Use the internal id to create the URL endpoint needed to access the metadata\n",
        "  url = \"https://conservancy.umn.edu/rest/items/\" + str(internal_id) + \"/metadata\"\n",
        "\n",
        "  #Read in the content at the metadata endpoint\n",
        "  response = urllib.request.urlopen(url)\n",
        "  soup = BeautifulSoup(response, 'lxml')\n",
        "  metadata = soup.p.string\n",
        "  list_metadata = eval(metadata.replace('null', '\"null\"'))\n",
        "\n",
        "  #Create the original metadata section of the log\n",
        "  metadata_string = \"\"\n",
        "  for x in range(len(list_metadata)):\n",
        "      metadata_string += list_metadata[x]['key'] + \" : \" + list_metadata[x]['value'] +\"\\n\"\n",
        "      #Create variables for a few specific metadata elements (title and handle) to use in the log header\n",
        "      if list_metadata[x]['key']=='dc.title':\n",
        "          title = list_metadata[x]['value']\n",
        "      if list_metadata[x]['key']=='dc.identifier.uri':\n",
        "          handle_uri = list_metadata[x]['value']\n",
        "      if list_metadata[x]['key'] == 'dc.date.available':\n",
        "          date_split = list_metadata[x]['value'].split(\"T\")\n",
        "          date_available = date_split[0] \n",
        "\n",
        "  ###Add the bitstream and metadata lists to the template metadata log text\n",
        "  metadata_log_template = \"Curation log for: \" + title + \"\"\"\n",
        "Handle: \"\"\" + handle_uri + \"\"\"\n",
        "Corresponding researcher:\n",
        "Curator:\n",
        "Metadata log created: \"\"\" + str(datetime.now().strftime(\"%Y-%m-%d\")) + \" (Dataset published: \" + date_available + \")\" + \"\"\"\n",
        "\\n*************************************************\n",
        "Files received:\n",
        "*************************************************\\n\"\"\" + bitstream_string + \"\"\"\n",
        "*************************************************\n",
        "Changes made to files:\n",
        "*************************************************\n",
        "\n",
        "**************************************************\n",
        "Metadata Changes\n",
        "**************************************************\n",
        "\n",
        "**************************************************\n",
        "Correspondence Notes\n",
        "**************************************************\n",
        "\n",
        "*************************************************\n",
        "Other issues\n",
        "*************************************************\n",
        "\n",
        "*************************************************\n",
        "Original Metadata from Author:\n",
        "*************************************************\\n\"\"\"  + metadata_string\n",
        "\n",
        "  metadata_filename = (str(handle_split[1]) + \"_CuratorLog_\" + str(datetime.now().strftime(\"%Y%m%d\")) + \".txt\")\n",
        "  with open(metadata_filename, 'w') as f:\n",
        "    f.write(metadata_log_template)\n",
        "\n",
        "  try:\n",
        "    files.download(metadata_filename)\n",
        "    print(\"###################################\")\n",
        "    print(\"            SUCCESS!               \")\n",
        "    print(\"     Check your downloads folder   \")\n",
        "    print(\"###################################\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(str(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbO-HkE95ebi"
      },
      "source": [
        "## CREATE README FILE\n",
        "\n",
        "\n",
        "---\n",
        "Run this block of code to create a readme file that will populate with the existing information on the record. By default, this file will be saved to your Downloads folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqe8I9OE6Pw7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "script name: automated_readme.py\n",
        "\n",
        "inputs: DRUM handle URL (i.e. https://hdl.handle.net/11299/226188 or https://conservancy.umn.edu/handle/11299/226188)\n",
        "output: A readme file in TXT format \n",
        "\n",
        "description: This script generates a readme from the information entered by\n",
        "researchers during the submission process to the Data Repository for the\n",
        "University of Minnesota (DRUM). It is based on a readme template originally\n",
        "developed at Cornell Univeristy.\n",
        "\n",
        "last modified: September 2022\n",
        "author: Melinda Kernik\n",
        "\"\"\"\n",
        "\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "from string import Template\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "#Use the handle URL to construct a URL to get to the Dspace endpoint for the item  \n",
        "handle_split = handle_url.split (\"/\") [-2:]\n",
        "handle = str(handle_split[0]) + \"/\" + str(handle_split[1])\n",
        "url = \"https://conservancy.umn.edu/rest/handle/\" + handle\n",
        "\n",
        "def validate_input(url):\n",
        "  \"\"\"Test whether the API for the DRUM item can be opened\"\"\"\n",
        "  valid_url = False\n",
        "  try:\n",
        "    response = urllib.request.urlopen(url)\n",
        "    valid_url = True\n",
        "    return valid_url\n",
        "  except Exception as e:\n",
        "    print (url + \" could not be opened. (\" + str(e) + \")\")\n",
        "\n",
        "#If the API can be reached, continue generating a Readme\n",
        "valid = validate_input(url)\n",
        "if valid:\n",
        "  response = urllib.request.urlopen(url)\n",
        "  #Read in the content at the endpoint and get the internal id for the item\n",
        "  presoup = BeautifulSoup(response, 'lxml')\n",
        "  item_info = presoup.p.text\n",
        "  item_dict = eval(item_info.replace('null', '\"null\"'))\n",
        "\n",
        "  internal_id = item_dict[\"id\"]\n",
        "\n",
        "  #Use the internal id to create the URL endpoint needed to access the metadata\n",
        "  url = \"https://conservancy.umn.edu/rest/items/\" + str(internal_id) + \"/metadata\"\n",
        "\n",
        "\n",
        "  ###Get metadata from the submission\n",
        "\n",
        "  #Read in the content at the metadata endpoint\n",
        "  response = urllib.request.urlopen(url)\n",
        "  soup = BeautifulSoup(response, 'lxml')\n",
        "  metadata = soup.p.string\n",
        "  list_metadata = eval(metadata.replace('null', '\"null\"'))\n",
        "\n",
        "  #Create an dictionary to be filled with metadata values from the submission\n",
        "  metadata_dict = {'readme_date': str(datetime.now().strftime(\"%Y-%m-%d\")), \n",
        "                   'author_citation':\"\", 'year_published':\"\", 'url':\"\",\n",
        "                   'title':\"\",'date_published':\"\", 'authors':\"\", 'contact_author': \"\", 'date_collected':\"\", \n",
        "                   'spatial':\"\", 'abstract': \"\", 'license_info':\"\", 'publications':\"\", \n",
        "                   'funding':\"\", 'file_list':\"\"}\n",
        "\n",
        "  #Create lists and dictionaries to hold multi-valued metadata elements or values \n",
        "  #that need to be edited before being added to the dictionary\n",
        "  authors_list = []\n",
        "  referenceby = []\n",
        "  funders = []\n",
        "  rights_dict = {}\n",
        "  date_collected_dict = {}\n",
        "\n",
        "            \n",
        "  #For each metadata field in Dspace, check if it is something to be included in the readme.  \n",
        "  #If it is, add it to the metadata dictionary or to a list for further processing.  \n",
        "  for x in range(len(list_metadata)):\n",
        "\n",
        "      #If the record has been assigned a DOI, use that for the recommended citation. Otherwise, use the handle.\n",
        "      if list_metadata[x]['key'] == 'dc.identifier.doi':\n",
        "        metadata_dict ['url'] = list_metadata[x]['value']\n",
        "      else:\n",
        "        metadata_dict ['url'] = handle_url\n",
        "\n",
        "      ##General information\n",
        "      if list_metadata[x]['key'] == 'dc.title':\n",
        "          metadata_dict ['title'] = list_metadata[x]['value']\n",
        "      \n",
        "      if list_metadata[x]['key'] == 'dc.contributor.author':\n",
        "          authors_list.append(list_metadata[x]['value'])\n",
        "      \n",
        "      #Retrieve the last name of the contact person to be used in the filename \n",
        "      if list_metadata[x]['key'] == 'dc.contributor.contactname':\n",
        "          contact_name = list_metadata[x]['value']\n",
        "          contact_split = contact_name.split (\",\") [:]\n",
        "          contact_lastname = contact_split[0].replace(\" \", \"_\")\n",
        "\n",
        "      if list_metadata[x]['key'] == 'dc.contributor.contactemail':\n",
        "          contact_email = list_metadata[x]['value']    \n",
        "      \n",
        "      #Split the date field and use only YYYYMMDD, not exact time\n",
        "      if list_metadata[x]['key'] == 'dc.date.available':\n",
        "          date_split = list_metadata[x]['value'].split(\"T\")\n",
        "          metadata_dict ['date_published'] = date_split[0]\n",
        "          #Isolate the year published to use in the Readme filename\n",
        "          year_split = date_split[0].split(\"-\")\n",
        "          year_published = year_split[0]\n",
        "          metadata_dict ['year_published'] = year_published\n",
        "      \n",
        "      if list_metadata[x]['key'] == 'dc.date.collectedbegin':\n",
        "          date_collected_dict['begin'] = list_metadata[x]['value']\n",
        "      if list_metadata[x]['key'] == 'dc.date.collectedend':\n",
        "          date_collected_dict['end'] = list_metadata[x]['value']\n",
        "      \n",
        "      if list_metadata[x]['key'] == 'dc.coverage.spatial':\n",
        "          metadata_dict ['spatial'] = list_metadata[x]['value']\n",
        "      \n",
        "      if list_metadata[x]['key'] == 'dc.description.sponsorship':\n",
        "          funders.append(list_metadata[x]['value'])  \n",
        "      \n",
        "      if list_metadata[x]['key'] == 'dc.description.abstract':\n",
        "          metadata_dict ['abstract'] = list_metadata[x]['value']            \n",
        "\n",
        "      #Sharing/Access Information\n",
        "      #Remove formatting from dc.rights field before adding it to the metadata dictionary\n",
        "      if list_metadata[x]['key'] == 'dc.rights':\n",
        "          rights_dict['rights'] = list_metadata[x]['value'].replace('\\r\\n', \" \")\n",
        "      if list_metadata[x]['key'] == 'dc.rights.uri':\n",
        "          rights_dict['rights_url'] = list_metadata[x]['value']\n",
        "      \n",
        "      if list_metadata[x]['key'] == 'dc.relation.isreferencedby':\n",
        "          referenceby.append(list_metadata[x]['value'])\n",
        "\n",
        "\n",
        "  ###Format multi-valued metadata elements to be added to the metadata dictionary      \n",
        "\n",
        "  #Format author information for use in the recommended citation\n",
        "  author_citation = \"\"\n",
        "  author_count = 0\n",
        "  for author in authors_list:\n",
        "    #Count while going through the list of authors.  When you reach the last author,\n",
        "    #use a period, otherwise separate using a semicolon. \n",
        "    author_count += 1\n",
        "    if author_count == len(authors_list):\n",
        "      author_citation += author + \".\"\n",
        "    else:\n",
        "      author_citation += author + \"; \"\n",
        "  metadata_dict ['author_citation'] = author_citation\n",
        "\n",
        "  #Format author information for use in the author contact section\n",
        "  author_string = \"\"\n",
        "  for author in authors_list:        \n",
        "      #Rearrange author name to be First Last instead of Last, First\n",
        "      author_split = author.split (\",\") [:]\n",
        "      author_firstLast = author_split[1] + \" \" + author_split[0]   \n",
        "      #If the author is the contact person, add their email address. If not, leave email blank.\n",
        "      if author == contact_name:\n",
        "          author_string += \"\\n\\tName: \" + author_firstLast + \"\\n\\tInstitution:\\n\\tEmail: \" + contact_email + \"\\n\\tORCID:\\n\\n\"\n",
        "      else:    \n",
        "          author_string += \"\\n\\tName: \" + author_firstLast + \"\\n\\tInstitution:\\n\\tEmail:\\n\\tORCID:\\n\\n\"\n",
        "  metadata_dict ['authors'] = author_string\n",
        "\n",
        "  try:\n",
        "    contact_author_string = \"\\tAuthor Contact: \" + contact_split[1] + \" \" + contact_split[0] + \" (\" + contact_email + \")\"\n",
        "  except:\n",
        "    contact_author_string = \"\\tAuthor Contact: \" + contact_name + \" (\" + contact_email + \")\"\n",
        "  metadata_dict ['contact_author'] = contact_author_string\n",
        "\n",
        "  funders_string = \"\"\n",
        "  for funder in funders:\n",
        "      funders_string += \"\\t\" + funder + \"\\n\"\n",
        "  metadata_dict ['funding'] = funders_string\n",
        "\n",
        "  publications_string = \"\"\n",
        "  for item in referenceby:\n",
        "      publications_string += item + \"\\n\\n\"\n",
        "  metadata_dict ['publications'] = publications_string\n",
        "\n",
        "\n",
        "  ## Add together multiple Dspace fields to be used in one section of the readme \n",
        "  if date_collected_dict: \n",
        "      metadata_dict ['date_collected'] = str(date_collected_dict['begin']) + \" to \" + str(date_collected_dict['end'])\n",
        "\n",
        "  if rights_dict: \n",
        "      try:\n",
        "        rights_string = rights_dict['rights'] + \" (\" + rights_dict[\"rights_url\"] + \")\"\n",
        "        metadata_dict ['license_info'] = rights_string\n",
        "      except:\n",
        "        metadata_dict ['license_info'] = rights_dict['rights']\n",
        "\n",
        "\n",
        "  ###Get item bitstream information from the submission\n",
        "\n",
        "  #Read in the content at the bitstream endpoint. Default limit is 20 items per page.  \n",
        "  #Extended to 250 to account for larger data submissions.\n",
        "  url = \"https://conservancy.umn.edu/rest/items/\" + str(internal_id) + \"/bitstreams?limit=250\"\n",
        "  response = urllib.request.urlopen(url)\n",
        "  item_soup = BeautifulSoup(response, 'lxml')\n",
        "  bitstream = item_soup.p.text\n",
        "  list_bitstream = eval(bitstream.replace('null', '\"null\"'))\n",
        "\n",
        "  #Create the \"File List\" section of the readme and add it to the metadata dictionary\n",
        "  file_list_string = \"File List\\n\\n\"\n",
        "  for x in list_bitstream:\n",
        "      if x['bundleName'] == \"ORIGINAL\":\n",
        "          file_list_string += (\"\\tFilename: \" + x['name'] +\" \\n\\tShort description: \" + x['description'] + \"\\n\\n\")\n",
        "  metadata_dict ['file_list'] = file_list_string\n",
        "\n",
        "\n",
        "\n",
        "  ###Insert metadata elements from the submission into the template readme text\n",
        "  readme_template = Template(\n",
        "\"\"\"This readme.txt file was generated on ${readme_date} by <Name>\n",
        "Recommended citation for the data: ${author_citation} (${year_published}). ${title}. Retrieved from the Data Repository for the University of Minnesota. ${url}.\\n\n",
        "-------------------\n",
        "GENERAL INFORMATION\n",
        "-------------------\\n\n",
        "1. Title of Dataset: ${title}\\n\n",
        "2. Author Information\\n\\n${contact_author}\\n${authors}\n",
        "3. Date published or finalized for release: ${date_published}\\n\\n\n",
        "4. Date of data collection (single date, range, approximate date): ${date_collected}\\n\\n\n",
        "5. Geographic location of data collection (where was data collected?): ${spatial}\\n\\n\n",
        "6. Information about funding sources that supported the collection of the data:\\n${funding}\\n\n",
        "7. Overview of the data (abstract):\\n${abstract}\\n\\n\\n\\n\n",
        "--------------------------\n",
        "SHARING/ACCESS INFORMATION\n",
        "--------------------------\\n\n",
        "1. Licenses/restrictions placed on the data: ${license_info}\\n\n",
        "2. Links to publications that cite or use the data:\\n${publications}\n",
        "3. Was data derived from another source?\n",
        "\\tIf yes, list source(s):\\n\n",
        "4. Terms of Use: Data Repository for the U of Minnesota (DRUM) By using these files, users agree to the Terms of Use. https://conservancy.umn.edu/pages/drum/policies/#terms-of-use\\n\\n\\n\\n\n",
        "---------------------\n",
        "DATA & FILE OVERVIEW\n",
        "---------------------\\n\n",
        "${file_list}\\n\n",
        "2. Relationship between files:\\n\\n\n",
        "--------------------------\n",
        "METHODOLOGICAL INFORMATION\n",
        "--------------------------\\n\n",
        "1. Description of methods used for collection/generation of data:\\n\\n\n",
        "2. Methods for processing the data: <describe how the submitted data were generated from the raw or collected data>\\n\\n\n",
        "3. Instrument- or software-specific information needed to interpret the data:\\n\\n\n",
        "4. Standards and calibration information, if appropriate:\\n\\n\n",
        "5. Environmental/experimental conditions:\\n\\n\n",
        "6. Describe any quality-assurance procedures performed on the data:\\n\\n\n",
        "7. People involved with sample collection, processing, analysis and/or submission:\\n\\n\\n\\n\"\"\")\n",
        "\n",
        "  #Replace variables in the template with the information from the metadata dictionary\n",
        "  readme_string = readme_template.substitute(metadata_dict)\n",
        "\n",
        "\n",
        "  ###Add a data_specific section to the readme for each spreadsheet file\n",
        "  #Make a list of all \"Original\" bitstream items with \".csv\" or \".xlsx\" in the name \n",
        "  spreadsheets = []\n",
        "  data_specific_string = \"\"\n",
        "  for x in list_bitstream:\n",
        "      if x['bundleName'] == \"ORIGINAL\":\n",
        "          if \".csv\" in x['name']:\n",
        "              spreadsheets.append(x['name'])\n",
        "          #Will pick up a range of Excel formats including .xls, .xlsx, and .xlsm\n",
        "          if \".xls\" in x['name']:\n",
        "              spreadsheets.append(x['name'])\n",
        "\n",
        "  #If there are no files with .csv or .xls extensions in the submission, add a \n",
        "  #placeholder \"[FILENAME]\" so that there will be one example section\n",
        "  if not spreadsheets:\n",
        "      spreadsheets.append(\"[FILENAME]\")\n",
        "\n",
        "  for item in spreadsheets:\n",
        "      data_specific_string += \"\"\"-----------------------------------------\n",
        "DATA-SPECIFIC INFORMATION FOR: \"\"\" + item + \"\"\"\\n-----------------------------------------\\n\n",
        "1. Number of variables:\\n\n",
        "2. Number of cases/rows:\\n\n",
        "3. Missing data codes:\\n\n",
        "\\tCode/symbol\\tDefinition\n",
        "\\tCode/symbol\\tDefinition\\n\n",
        "4. Variable List\\n\n",
        "\\tA. Name: <variable name>\n",
        "\\t   Description: <description of the variable>\n",
        "\\t\\tValue labels if appropriate\\n\n",
        "\\tB. Name: <variable name>\n",
        "\\t   Description: <description of the variable>\n",
        "\\t\\tValue labels if appropriate\\n\\n\\n\\n\"\"\"\n",
        "\n",
        "  #Add the data-specific section(s) onto the end of the readme\n",
        "  readme_full_string = readme_string + data_specific_string\n",
        "\n",
        "  #Create the file name using contact person's last name and the year the submission was published\n",
        "  #If contact person has not been identified create a file name with just the year published\n",
        "  try:\n",
        "    readme_filename = (\"Readme_\" + contact_lastname + \"_\" + year_published + \".txt\")\n",
        "  except:\n",
        "    readme_filename = (\"Readme_\" + year_published + \".txt\")\n",
        "    print (\"The name given for the contact author did not exactly match any of the names in the author list. Their contact info will need to be added to the Readme manually.\")\n",
        "  \n",
        "  #Generate the Readme\n",
        "  with open(readme_filename, 'w') as f:\n",
        "    f.write(readme_full_string)\n",
        "\n",
        "  try:\n",
        "    files.download(readme_filename)\n",
        "    print(\"###################################\")\n",
        "    print(\"            SUCCESS!               \")\n",
        "    print(\"     Check your downloads folder   \")\n",
        "    print(\"###################################\")\n",
        "  except Exception as e:\n",
        "    print(str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCvSHT2z5VsD"
      },
      "source": [
        "## CREATE XML FILE\n",
        "\n",
        "\n",
        "---\n",
        "Run this block of code to create an XML file that is formatted in the DataCite metadata schema, based on the information on the record. This file will be saved in an XML format in your Downloads folder, and will need to be uploaded to DataCite to create a DOI for the record. \n",
        "\n",
        "> [Instructions for uploading the file to DataCite](https://docs.google.com/document/d/16CVkUWrRRStqErDS_L5DRoAaLOEZlAoJBtiiOKFCirE/edit#)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bhFco3H1ngM"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "script name: datacite_xml.py\n",
        "\n",
        "inputs: DRUM handle URL (i.e. https://hdl.handle.net/11299/226188 or https://conservancy.umn.edu/handle/11299/226188)\n",
        "output: An XML file in DataCite  \n",
        "\n",
        "description: This script generates an XML file using the information from the DRUM record. \n",
        "The XML file is formatted in the DataCite Metadata Schema 4.4, and can generate a DOI when \n",
        "uploaded to DataCite.\n",
        "\n",
        "last modified: July 2022\n",
        "author: Valerie Collins & Melinda Kernik\n",
        "\"\"\"\n",
        "\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "from string import Template\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "#Use the handle URL to construct a URL to get to the Dspace endpoint for the item  \n",
        "handle_split = handle_url.split (\"/\") [-2:]\n",
        "handle = str(handle_split[0]) + \"/\" + str(handle_split[1])\n",
        "url = \"https://conservancy.umn.edu/rest/handle/\" + handle\n",
        "\n",
        "def validate_input(url):\n",
        "  \"\"\"Test whether the API for the DRUM item can be opened\"\"\"\n",
        "  valid_url = False\n",
        "  try:\n",
        "    response = urllib.request.urlopen(url)\n",
        "    valid_url = True\n",
        "    return valid_url\n",
        "  except Exception as e:\n",
        "    print (url + \" could not be opened. (\" + str(e) + \")\")\n",
        "\n",
        "#If the API can be reached, continue generating a XML    \n",
        "valid = validate_input(url)\n",
        "if valid:\n",
        "\n",
        "  #Read in the content at the item API endpoint and get the internal id for the item\n",
        "  response = urllib.request.urlopen(url)\n",
        "  presoup = BeautifulSoup(response, 'lxml')\n",
        "  item_info = presoup.p.text\n",
        "  item_dict = eval(item_info.replace('null', '\"null\"'))\n",
        "\n",
        "  internal_id = item_dict[\"id\"]\n",
        "\n",
        "  metadata_url = \"https://conservancy.umn.edu/rest/items/\" + str(internal_id) + \"/metadata\"\n",
        "\n",
        "  #Read in the content at the metadata endpoint\n",
        "  response = urllib.request.urlopen(metadata_url)\n",
        "  soup = BeautifulSoup(response, 'lxml')\n",
        "  metadata = soup.p.string\n",
        "  list_metadata = eval(metadata.replace('null', '\"null\"'))\n",
        "\n",
        "\n",
        "  #Create lists to hold the multi-valued metadata elements & empty strings for values that might not exist\n",
        "  authors_list = []\n",
        "  subjects_list = []\n",
        "  rights_string = \"\"\n",
        "  description_string = \"\"\n",
        "  technical_desc_string = \"\"\n",
        "  abstract_string = \"\"\n",
        "  license_text = \"\"\n",
        "  license_url = \"\"\n",
        "\n",
        "  #For each metadata field in Dspace, check if it is something to be included in the XML.\n",
        "  #If it is, save it to a variable.\n",
        "  for x in range(len(list_metadata)):\n",
        "      ##General information\n",
        "      if list_metadata[x]['key'] == 'dc.title':\n",
        "          title = list_metadata[x]['value']\n",
        "\n",
        "      if list_metadata[x]['key'] == 'dc.contributor.author':\n",
        "          authors_list.append(list_metadata[x]['value'])\n",
        "\n",
        "      if list_metadata[x]['key'] == 'dc.subject':\n",
        "          subjects_list.append(list_metadata[x]['value'])\n",
        "\n",
        "      if list_metadata[x]['key'] == 'dc.date.available':\n",
        "          date_available = list_metadata[x]['value']\n",
        "\n",
        "      if list_metadata[x]['key'] == 'dc.identifier.uri':\n",
        "          alt_id = list_metadata[x]['value']\n",
        "\n",
        "      ##checking for variables that may not exist\n",
        "      if list_metadata[x]['key'] == \"dc.description\":\n",
        "          technical_description = list_metadata[x]['value']\n",
        "          technical_desc_string = \"\"\"\n",
        "<description descriptionType=\"TechnicalInfo\">\"\"\"+technical_description+\"\"\"</description>\"\"\"\n",
        "\n",
        "      if list_metadata[x]['key'] == 'dc.description.abstract':\n",
        "          abstract = list_metadata[x]['value']\n",
        "          abstract_string = \"\"\"\n",
        "<description descriptionType=\"Abstract\">\"\"\" + abstract + \"\"\"</description>\"\"\"\n",
        "\n",
        "      #if abstract or description element exists, then build the description block\n",
        "      if technical_desc_string != \"\" or abstract_string != \"\":\n",
        "          description_string = \"\"\"\n",
        "<descriptions>\"\"\" + abstract_string + technical_desc_string + \"\"\"\n",
        "</descriptions>\"\"\"\n",
        "\n",
        "\n",
        "      if list_metadata[x]['key'] == 'dc.rights':\n",
        "          license_text = list_metadata[x]['value']\n",
        "\n",
        "      if list_metadata[x]['key'] == 'dc.rights.uri':\n",
        "          license_url = list_metadata[x]['value']\n",
        "\n",
        "      #license text and URI must both be present to build the rights block\n",
        "      if license_text != \"\" and license_url != \"\":\n",
        "          rights_string = \"\"\"\n",
        "<rightsList>\n",
        "  <rights rightsURI=\\\"\"\"\"+license_url+\"\"\"\\\">\"\"\"+license_text+\"\"\"</rights>\n",
        "</rightsList>\"\"\"\n",
        "\n",
        "\n",
        "  ### Format multi-valued metadata element \"author\" to be added to the XML\n",
        "  author_string = \"\"\n",
        "  for author in authors_list:\n",
        "      #Split up author name\n",
        "      author_split = author.split (\", \") [:]\n",
        "      author_first = author_split[1]\n",
        "      author_last = author_split[0].strip()\n",
        "      #loop through authors and append each new XML <creator> block to author_string\n",
        "      author_string += \"\"\"\n",
        "<creator>\n",
        "  <creatorName nameType=\"Personal\">\"\"\" + author + \"\"\"</creatorName>\n",
        "  <givenName>\"\"\" + author_first + \"\"\"</givenname>\n",
        "  <familyName>\"\"\" + author_last + \"\"\"</familyname>\n",
        "</creator>\"\"\"\n",
        "\n",
        "  #format <subject> block if subjects exist\n",
        "  subject_string = \"\"\n",
        "  if bool(subjects_list):\n",
        "      subjects = \"\"\n",
        "      #loop through and build out subject block\n",
        "      for subject in subjects_list:\n",
        "          subjects += \"\"\"\n",
        "  <subject>\"\"\" + subject + \"\"\"</subject> \"\"\"\n",
        "      #add subject blocks to outer tags\n",
        "      subject_string = \"\"\"\n",
        "<subjects>\"\"\" + subjects + \"\"\"\\n</subjects>\"\"\"\n",
        "\n",
        "  #schema template\n",
        "  datacite_schema = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<resource xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://datacite.org/schema/kernel-4\" xsi:schemaLocation=\"http://datacite.org/schema/kernel-4 https://schema.datacite.org/meta/kernel-4.4/metadata.xsd\">\n",
        "<identifier identifierType=\"DOI\"></identifier>\n",
        "<creators> \"\"\" + author_string + \"\"\"\n",
        "</creators>\n",
        "<titles>\n",
        "  <title>\"\"\" + title + \"\"\"</title>\n",
        "</titles>\n",
        "<publisher>Data Repository for the University of Minnesota (DRUM)</publisher>\n",
        "<publicationYear>\"\"\" + str(datetime.now().strftime(\"%Y\")) + \"\"\"</publicationYear>\n",
        "<resourceType resourceTypeGeneral=\"Dataset\"/>\"\"\" + subject_string + \"\"\"\n",
        "<dates>\n",
        "  <date dateType=\"Available\">\"\"\" + date_available + \"\"\"</date>\n",
        "</dates>\n",
        "<alternateIdentifiers>\n",
        "  <alternateIdentifier alternateIdentifierType=\"Handle\">\"\"\" + alt_id + \"\"\"</alternateIdentifier>\n",
        "</alternateIdentifiers>\n",
        "<sizes/>\n",
        "<formats/>\n",
        "<version/>\"\"\" + rights_string + description_string + \"\"\"\n",
        "</resource>\"\"\"\n",
        "\n",
        "\n",
        "  schema_file_name = (str(handle_split[1]) + \"_doi_xml.xml\")\n",
        "\n",
        "  with open(schema_file_name, 'w') as f:\n",
        "    f.write(datacite_schema)\n",
        "\n",
        "  try:\n",
        "    files.download(schema_file_name)\n",
        "    print(\"###################################\")\n",
        "    print(\"            SUCCESS!               \")\n",
        "    print(\"     Check your downloads folder   \")\n",
        "    print(\"###################################\")\n",
        "  except Exception as e:\n",
        "    print(str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoWgjQhF45d3"
      },
      "source": [
        "# Known Issues and Limitations\n",
        "---\n",
        "\n",
        "DRUM curators can find a full list of known issues and limitations with these tools for our workflows in [this Google Drive document](https://docs.google.com/document/d/1CHwFyh4679QYuL7nouN5F9SxGK7I1-3W2B1i83lxm-o/edit#heading=h.3aytv8au0zxl)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BjYCAMxkabP"
      },
      "source": [
        "# Download All Files from Record\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "We have created a separate, standalone tool that can be used to download all files from a record to a directory on your computer. Files will be downloaded as they are listed on the DRUM record, and will not be zipped into one package. This tool works on Windows environments only.\n",
        "\n",
        "You can access the .exe file [here](https://z.umn.edu/DRUM_downloadFiles).  Download and extract the folder.  Click to open \"DRUM_downloadFiles.exe.\"  You will have to override your anti-virus software if it's your first time opening the file. \n",
        "\n",
        "The python script version of the tool is also available on [GitHub](https://github.com/mkernik/drum_tools/blob/main/DRUM_downloadFiles.py)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SYvpjDCV5j6t",
        "MbO-HkE95ebi",
        "NCvSHT2z5VsD"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}